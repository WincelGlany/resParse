import streamlit as st
import pandas as pd
import openai
import base64
import fitz
import os
import uuid
import time

from bs4 import BeautifulSoup
from fuzzywuzzy import fuzz
from database import database
from pandasai import PandasAI
from pandasai.llm.openai import OpenAI
from PIL import Image
from streamlit_option_menu import option_menu
from streamlit_extras.switch_page_button import switch_page
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager

# Set the Streamlit page configuration
st.set_page_config(layout="wide")

# API Key
key1 = 'OpenAI_API_Key1'
key2 = 'OpenAI_API_Key2'


openai.api_key = key1
# Instantiate a LLM
llm = OpenAI(api_token=key2)
pandas_ai = PandasAI(llm)

# Display the pdf
def display_pdf(pdf_file_path):
    with open(pdf_file_path, 'rb') as file:
        base64_pdf = base64.b64encode(file.read()).decode('utf-8')
    
    # CSS styling to center the PDF display
    centered_style = """
    <style>
    .centered {
        display: flex;
        justify-content: center;
        align-items: center;
        margin: 0 auto;
        max-width: 700px;
        height: 1000px;
    }
    </style>
    """
    
    # PDF display code with CSS styling
    resume_pdf_display = f'<div class="centered"><iframe src="data:application/pdf;base64,{base64_pdf}" width="700" height="1000" type="application/pdf"></iframe></div>'
    
    # Display the PDF
    st.markdown(centered_style, unsafe_allow_html=True)
    st.markdown(resume_pdf_display, unsafe_allow_html=True)

# Initialising the model 
def get_completion(prompt, model="gpt-3.5-turbo",temperature=0):
    messages = [{"role": "user", "content": prompt}]
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=0,
    )
    return response.choices[0].message["content"] 
    
# Create prompt for true resume check
def generate_prompt(text, prompt_type):
    if prompt_type == "true_check":
        prompt = f"""
        ```{text}```
        Check if the text given is generated by chatGPT or any other AI tools. If yes return "A"
        """
    elif prompt_type == "summaries_prompt":
        prompt = f"""
        ```{text}```
        This is a dictionary containing information about a person.
        
        Detailed Explanation:\n 
        
        Note: 
        exclude keyword
        Also exclude elements that have "Nan"
        Put all details in bulletine points one below the other
        
        """
    elif prompt_type == "ner_extraction":
        prompt = f"""
        ```{text}```
        From the text extract NER (named entity recognition)

        Things to extract:
        1. Name
        2. Email ID
        3. Phone number 
        4. LinkedIn ID
        5. Skills (eg, programming, marketing)
        6. Tools (Linux, ms, python, java)
        7. Experience 
        8. Project name
        9. College name
        10. Referrals name
        11. Referrals phone number
        12. Referrals email
        13. Location
        14. Companies worked at
        15. Designation 
        16. Keywords (give all technical skills, tools, and any important details found in the text separated by commas, e.g., C, C++, Python, Java, programming, etc.)

        Separate the details with :
        Skills and Tools details should all be in separate "" and separated by a comma
        If any of the details isn't existing in the text, return "Nan"
        Put all these details into a dictionary 
        """
    elif prompt_type == "linkedIn_extraction":
            prompt = f"""
            ```{text}```

            From the texts given extract ner(named entity recognition)

            Things to extract
            Name: 
            About:
            Education:
            Projects:
            Skills:
            Tools:
            Experience:

            Only give the keywords
            All details that are in the form of a list should all be in seperate "" and seperated by a comma
            For name only give the first one only
            Put all these details into a dictionary 

            """
    
    else:
        raise ValueError("Invalid prompt_type. Use 'true_check', 'summaries_prompt', or 'ner_extraction'.")

    return get_completion(prompt)

# Function to delete records from the database and files from the folder
def delete_records_and_files():
    try:
        # Database operations
        db = database()
        cursor = db.cursor()
        
        cursor.execute("DELETE FROM resume_text WHERE unique_id LIKE '2000%'")
        cursor.execute("DELETE FROM resume_detail WHERE unique_id LIKE '2000%'")
        cursor.execute("DELETE FROM resume_keyword WHERE unique_id LIKE '2000%'")

        cursor.execute("ALTER TABLE resume_text AUTO_INCREMENT = 1")
        cursor.execute("ALTER TABLE resume_detail AUTO_INCREMENT = 1")
        cursor.execute("ALTER TABLE resume_keyword AUTO_INCREMENT = 1")
        
        db.commit()

        cursor.close()
        db.close()

        # Folder path for file deletion
        folder_path = './Resumes'

        # Get file names in the folder
        file_names = os.listdir(folder_path)

        # Iterate over the file names and delete each file
        for file_name in file_names:
            file_path = os.path.join(folder_path, file_name)
            if os.path.isfile(file_path) and file_name != ".gitkeep":
                os.remove(file_path)
                # st.write(f"Deleted file: {file_name}")

        st.sidebar.write("Database records and files deleted successfully.")

    except Exception as e:
        st.error(f"An error occurred: {e}")

# Define your sidebar content
def sidebar():
    st.sidebar.image("logo2.png", use_column_width=True)
    st.sidebar.markdown("*Powered by N2G*")
    st.sidebar.markdown("rezSearch - Streamlining the Recruitment process using Artificial Intelligence!")
    # st.sidebar.markdown("<br>", unsafe_allow_html=True)
    # st.sidebar.markdown("<br>", unsafe_allow_html=True)
    # st.sidebar.markdown("<br>", unsafe_allow_html=True)
    
    with st.sidebar.expander("Delete all resume details"):
        if st.button("Delete"):
            delete_records_and_files()
            
    st.sidebar.markdown(
        """
        <style>
        .sidebar .st-expander {
            background-color: #ff0000; /* Change the color to your desired color */
        }
        </style>
        """,
        unsafe_allow_html=True
    )


# Define your main content
def main_content():
    st.markdown("<h1 style='text-align: center;'>rezSearch - Compelete Tool for Recruitment Analysis</h1>", unsafe_allow_html=True)
    st.markdown("<br>", unsafe_allow_html=True) 
    st.markdown("<br>", unsafe_allow_html=True) 
    selected = option_menu(
        menu_title=None,
        options=["Home", "JobAnalyzer", "LinkedInGPT", "RezMiner", "RezRanker", "RezGPT"],
        icons=['house', 'graph-up-arrow', 'info', 'tags', 'bar-chart', 'search'],
        menu_icon="cast",
        orientation="horizontal",
        styles={
            "nav-link": {
                "text-align": "left",
                "--hover-color": "#eee",
                "--select-color": "maroon",
            }
        }
    )
    if selected == "Home":
        # Content for homepage
        st.markdown("**Welcome to rezSearch** – Welcome to rezSearch, your one-stop solution for streamlined resume processing and enhanced talent acquisition! Our platform offers a range of easy and efficient resume parsing solutions designed to save you time and effort in the hiring process. With our cutting-edge technology, we can quickly extract relevant information from resumes, such as skills, qualifications, work experience, and education, and organize it in a structured format, eliminating the need for manual data entry. This ensures that you can focus on evaluating candidates rather than spending hours sifting through piles of resumes.")

        st.markdown("**rezSearch leverages Aritificial Intellegence and provide few amazing featues like:**")
        st.markdown("- **JobAnalyzer:** Introducing our JobAnalyzer, where finding your dream job is simple and efficient. With our vast and diverse range of job opportunities from leading companies across various industries, you can easily browse through job listings, filter results based on your preferences, and apply with just a few clicks.")
        st.markdown("- **LinkedInGPT:** Introducing LinkedInGPT, a powerful tool that enhances your recruitment process by providing deeper insights into potential candidates. With our platform, you can effortlessly access comprehensive information about your recruits, including their professional backgrounds, skills, career achievements, and endorsements.")
        st.markdown("- **RezMiner:** Introducing RezMiner, the game-changer in resume screening with its user-friendly platform that allows effortless uploading of resumes and seamless extraction of key details. Utilizing advanced parsing technology, it efficiently organizes candidates' qualifications, experience, and skills, streamlining the talent acquisition process.")
        st.markdown("- **RezRanker:** Introducing our RezRanker, your ultimate solution for quick and efficient keyword-based resume exploration. With our intuitive platform, you can type any keyword and instantly see all resumes from our extensive database, with the most relevant matches highlighted.")
        st.markdown("- **RezGPT:** Introducing RezMiner, the ultimate destination for gaining deeper insights into your candidates' qualifications, experience, and skills. Engage in real-time chat with our intelligent platform to uncover valuable details and have insightful conversations about potential candidates. ResumeMiner provides a comprehensive solution to help you make well-informed hiring decisions and find the best match for your organization.")

        st.markdown("By harnessing the power of cutting-edge resume parsing technology, resuSearch empowers hiring teams and recruiters with a comprehensive understanding of candidates' qualifications, skills, and experiences. With resuSearch, you can streamline your resume processing, saving valuable time and resources during the talent acquisition process. Our intelligent parsing algorithm accurately extracts and organizes vital information from resumes, providing a granular and structured overview of each candidate's potential fit for the job.")

    
    # Database    
    db = database()

    # Create a cursor
    cursor = db.cursor()
    
    if selected == "RezMiner":
        st.markdown("<h2>RezMiner</h2>", unsafe_allow_html=True)
        st.markdown("Welcome to RezMiner, we revolutionize the resume screening process with its user-friendly platform, enabling effortless uploading of resumes and seamless extraction of key details. With advanced parsing technology, it efficiently organizes candidates' qualifications, experience, and skills, streamlining talent acquisition. Say goodbye to manual data entry and hello to a smarter, more efficient way of finding the perfect candidates for your organization with ResumeMiner.")

        # Tooltip  
        working_rezminer = ''' **How it works**  \n - Drag and Drop Resumes  \n - Wait for Processing  \n - View Extracted Details''' 
        
        pdf_file = st.file_uploader("Upload your Resume", type=['pdf'], help=working_rezminer)

        # Check if a file is uploaded
        if pdf_file is not None:
            # Saving pdfs
            save_resume_path = './Resumes/' + pdf_file.name
            with open(save_resume_path, 'wb') as f:
                f.write(pdf_file.getbuffer())
            display_pdf(save_resume_path)
            
            # Read the PDF file
            pdf_data = pdf_file.read()

            # Load the PDF document
            pdf_document = fitz.open(stream=pdf_data, filetype="pdf")

            # Extract text from each page
            text = ""
            for page in pdf_document:
                text += page.get_text()
                
            
            # Clean the text
            text = text.strip()
            text = ' '.join(text.split())
            
            # NER Extraction from chatGPT
            response = generate_prompt(text, "ner_extraction")
            
            st.markdown("<br>", unsafe_allow_html=True)
            st.markdown("<h3>Resume Analysis</h3>", unsafe_allow_html=True)
            st.markdown("<br>", unsafe_allow_html=True)
            st.write(generate_prompt(response, "summaries_prompt"))
            
            df_ner = pd.DataFrame(eval(response), index=[0])
            
            # Generate a unique ID and store it in the list
            unique_id = '2000' + str(uuid.uuid4())
                
            # st.dataframe(eval(response))
            
            # Check if the PDF already exists in the database
            select_query = "SELECT * FROM resume_text WHERE text = %s"
            cursor.execute(select_query, (text,))
            existing_data = cursor.fetchone()
            
            if existing_data:
                # PDF already exists, skip insertion
                st.write("PDF already uploaded. Skipping insertion.")
            else:
                # Insert the extracted text into the database
                # Insert in table name resume_text
                insert_query = "INSERT INTO resume_text (text, unique_id) VALUES (%s, %s)"
                cursor.execute(insert_query, (text, unique_id))
                db.commit()
                
                # Insert in table name resume_details
                for index, row in df_ner.iterrows():
                    insert_query = "INSERT INTO resume_detail (name, email, phone_number, linkedin_id, skills, tools, experience, project, college_name, referrals_name, referrals_phone_number, referrals_email, location, companies_worked_at, designation, unique_id) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)"
                    cursor.execute(insert_query, (row['Name'], row['Email ID'], row['Phone number'], 
                                                row['LinkedIn ID'], row['Skills'], row['Tools'], 
                                                row['Experience'], row['Project name'], row['College name'], row['Referrals name'], 
                                                row['Referrals phone number'], row['Referrals email'], row['Location'], 
                                                row['Companies worked at'], row['Designation'], unique_id))
                    db.commit()
                    
                # Insert in table name resume_keyword
                for index, row in df_ner.iterrows():
                    # Get the values from the dataframe
                    keyword = row['Keywords']
                    
                    # Insert the values into the table
                    insert_query = "INSERT INTO resume_keyword (filename, text, keyword, unique_id) VALUES (%s, %s, %s, %s)"
                    cursor.execute(insert_query, (save_resume_path, text, keyword, unique_id))
                    db.commit()
        
    if selected == "RezRanker":
        st.markdown("<h2>RezRanker</h2>", unsafe_allow_html=True)
        st.markdown("Welcome to our RezRanker, your ultimate solution for quick and efficient keyword-based resume exploration. With our intuitive platform, you have the power to type any keyword, and instantly, all resumes from our extensive database will be displayed, highlighting the most relevant matches.")
             
        query = "SELECT * FROM resume_keyword WHERE unique_id LIKE '2000%'"
        cursor.execute(query)

        # Fetch the column names from the cursor description
        columns = [col[0] for col in cursor.description]

        # Fetch the data
        db_data = cursor.fetchall()

        # Convert data to a dataframe
        data = pd.DataFrame(db_data, columns=columns)

        data['keyword'] = data['keyword'].astype(str)

        # Define function for fuzzy string matching
        def similarity_fuzzy(word1, word2):
            score = fuzz.ratio(word1, word2)
            d = score/100
            return d

        # Define function to get top resume matches
        def get_top_resumes(search_word, threshold):
            name = []
            words = []
            for i in range(len(data)):
                name.append(data.loc[i, "filename"])
                words.append(data.loc[i, "keyword"].split(","))
            
            i = 0
            score_dict = dict()
            for each_resume_words in words:
                fuzz_score = 0
                for word in each_resume_words: 
                    if similarity_fuzzy(word.lower(), search_word.lower()) > threshold:
                        fuzz_score += 1
                score_dict[name[i]] = fuzz_score
                i += 1
            ranked_dict = dict(sorted(score_dict.items(), key=lambda item: item[1], reverse=True))
            
            return ranked_dict
        
        # Tooltip  
        working_rezranker = ''' **How it works**  \n - Type Keyword  \n - Click Search Button  \n - View Extracted Details'''

        # Create search box and button
        search_word = st.text_input("Enter search term:", "", help=working_rezranker)
        threshold = st.slider("Select similarity threshold:", 0, 100, 70)
        #search_word = 'NLP'
        threshold = threshold/100

        submit_button = st.button("Search")

        if search_word != "" and submit_button:
            # Get top resume matches and display in a table
            top_resumes = get_top_resumes(search_word, threshold)
            if len(top_resumes) > 0:
                st.write("Top resume matches:")
                df_rank = pd.DataFrame(list(top_resumes.items()), columns=['filename', 'Score'])
                st.table(df_rank)

                # Create dropdown to view selected resume
                selected_resume = st.selectbox("Select a resume to view:", df_rank['filename'], key="resume_select")

                if selected_resume:
                    st.markdown("<br>", unsafe_allow_html=True)
                    st.markdown("<h3>Resume</h3>", unsafe_allow_html=True)
                    st.markdown("<br>", unsafe_allow_html=True)
                    display_pdf(selected_resume)
            else:
                st.write("No matching resumes found.")
        else:
            st.write("Please enter a search term.")
        
    if selected == "RezGPT":
        st.markdown("<h2>RezGPT</h2>", unsafe_allow_html=True)
        st.markdown("Welcome to RezGPT, your one-stop destination for insightful conversations and comprehensive information about your candidates. Engage in real-time chat with our intelligent platform to uncover valuable details and gain deeper insights into each candidate's qualifications, experience, and skills.")      
        
        query = "SELECT * FROM resume_detail WHERE unique_id LIKE '2000%'"
        cursor.execute(query)

        # Fetch the column names from the cursor description
        columns = [col[0] for col in cursor.description]

        # Fetch the data
        data = cursor.fetchall()
        
        # Convert data to a dataframe
        df_detail = pd.DataFrame(data, columns=columns)
        
        # Tooltip  
        working_rezgpt = ''' **How it works**  \n - Chat with us  \n - Wait for Processing  \n - View Extracted Details'''
        
        # Prompt the user for columns to display
        prompt_from_user = st.text_area("Send a message: ", help=working_rezgpt)

        if st.button("Generate:"):
            if prompt_from_user:
                with st.spinner("Generating response..."):
                    st.write(pandas_ai(df_detail, prompt=prompt_from_user))
            else:
                st.warning("Enter a prompt")

        
    if selected == "LinkedInGPT":
        st.markdown("<h2>LinkedInGPT</h2>", unsafe_allow_html=True)
        st.markdown("Welcome to our LinkedInGPT, a powerful tool designed to enhance your recruitment process by providing deeper insights into potential candidates. With our platform, you can now effortlessly access comprehensive information about your recruitees, from their professional backgrounds and skills to their career achievements and endorsements.")
            
        # Execute the SQL query
        cursor.execute("SELECT linkedin_id FROM resume_detail WHERE linkedin_id != 'Nan' AND unique_id LIKE '2000%'")

        # Fetch the data
        data_link = cursor.fetchall()

        # Convert data to a dataframe
        df_link = pd.DataFrame(data_link, columns=['Linkedin_profiles'])
        
        # Tooltip  
        working_linkedIn1 = ''' **How it works**  \n - Select the profile from drop down  \n - Wait for Processing  \n - View Extracted Details'''
        working_linkedIn2 = ''' **How it works**  \n - Upload a profile link  \n - Wait for Processing  \n - View Extracted Details'''

        # paste the URL 
        selected_profile = st.selectbox("Select a profile to view:", [""] + df_link['Linkedin_profiles'].tolist(), key="profile_select", help=working_linkedIn1)
        st.text("OR")
        profile_url = st.text_input("Enter the profile URL", "", help=working_linkedIn2)
        
        if selected_profile or profile_url:
            # Creating a webdriver instance
            chrome_options = Options()
            chrome_options.add_argument('--headless')

            driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)
            # driver = webdriver.Chrome()

            # Opening linkedIn's login page
            driver.get("https://linkedin.com/uas/login")

            # waiting for the page to load
            time.sleep(5)

            # entering username
            username = driver.find_element(By.ID, "username")

            # Enter Your Email Address
            username.send_keys("linkedIn_username")
            time.sleep(0.5)

            # entering password
            pword = driver.find_element(By.ID, "password")

            # Enter Your Password
            pword.send_keys("linkedIn_password")
            time.sleep(0.5)

            # Clicking on the log in button
            driver.find_element(By.XPATH, "//button[@type='submit']").click()
            
            if selected_profile != "":
                driver.get("https://www."+selected_profile)
                time.sleep(3)
            else:
                driver.get(profile_url)
                time.sleep(3)


            start = time.time()

            # will be used in the while loop
            initialScroll = 0
            finalScroll = 1000

            while True:
                driver.execute_script(f"window.scrollTo({initialScroll},{finalScroll})")
                
                # scrolls the window starting from
                initialScroll = finalScroll
                finalScroll += 1000

                time.sleep(3)
            
                end = time.time()
                
                if round(end - start) > 20:
                    break
                
            src = driver.page_source
            
            # Now using beautiful soup
            soup = BeautifulSoup(src, 'lxml')

            text_soup = soup.find_all('div',class_='display-flex align-items-center mr1 hoverable-link-text t-bold')
            text1_soup = soup.find('div', {'class': 'pv-text-details__left-panel'})

            combined_text = text_soup + [text1_soup]
            linkedIn_text = generate_prompt(combined_text, "linkedIn_extraction")
            
            st.markdown("<br>", unsafe_allow_html=True)
            st.markdown("<h3>LinkedIn Detail Analysis</h3>", unsafe_allow_html=True)
            st.markdown("<br>", unsafe_allow_html=True)
            st.write(generate_prompt(linkedIn_text, "summaries_prompt"))
    
    if selected == "JobAnalyzer":
        st.markdown("<h2>JobAnalyzer</h2>", unsafe_allow_html=True)
        st.markdown("Welcome to our JobAnalyzer, where finding your dream job is made simple and efficient. Our website offers a vast and diverse range of job opportunities from leading companies across various industries. With our user-friendly interface and powerful search tools, you can easily browse through job listings, filter results based on your preferences, and apply with just a few clicks.")
        
        # Creating a webdriver instance
        chrome_options = Options()
        chrome_options.add_argument('--headless=new')

        browser = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)
        
        # tooltip
        working_job = ''' **How it works**  \n - Search for job openings here  \n - Filter the job postings  \n - Wait for Processing  \n - View Extracted Details'''

        # query search on google jobs site with location mentioned
        query = st.text_input("What jobs are you looking for: ", help=working_job)
        query = '+'.join(query.lower().split())
        url = f"https://www.google.com/search?q={query}&oq=/&aqs=chrome..69i57j69i59j0i512j0i22i30i625l4j69i60.4543j0j7&sourceid=chrome&ie=UTF-8&ibp=htl;jobs&sa=X&ved=2ahUKEwjXsv-_iZP9AhVPRmwGHX5xDEsQutcGKAF6BAgPEAU&sxsrf=AJOqlzWGHNISzgpAUCZBmQA1mWXXt3I7gA:1676311105893#htivrt=jobs&htidocid=GS94rKdYQqQAAAAAAAAAAA%3D%3D&fpstate=tldetail"
        browser.get(url)
        page = BeautifulSoup(browser.page_source)

        if query != "":
            #job filter
            jobs_filter = page.find_all('div', {"data-facet": "job_family_1", "class": "eNr05b GbaVB ZkkK1e yUTMj k1U36b"})
            job_filter_list = [i['data-value'] for i in jobs_filter if not i['data-value'] == '__placeholder__']
            # Select job filter if needed
            words_jobs = set(job_filter_list)
            descriptions_jobs = words_jobs

            selected_options_jobs = st.sidebar.multiselect('Select Job Options', descriptions_jobs)

            #location filter
            location_filter=page.find_all('div',{"data-facet":"city","class":"eNr05b GbaVB ZkkK1e yUTMj k1U36b"})
            location_filter_list=[i['data-value'] for i in location_filter if not i['data-value']=='__placeholder__']
            location_filter_name=[i['data-name'] for i in location_filter if not i['data-value']=='__placeholder__']
            location_dict={location_filter_name[i]:location_filter_list[i] for i in range(len(location_filter_list))}
            #Select location filter if needed
            words_loc = location_filter_name
            descriptions_loc = words_loc
            
            selected_options_loc = st.sidebar.multiselect('Select Location', descriptions_loc)
            
            #employment type filter
            type_filter=page.find_all('div',{'data-facet':"employment_type","class":"eNr05b GbaVB ZkkK1e yUTMj k1U36b"})
            type_filter_list=[i['data-value'] for i in type_filter if not i['data-value']=='__placeholder__']
            #Select location filter if needed
            words_type = set(type_filter_list)
            descriptions_type = words_type
            
            selected_options_type = st.sidebar.multiselect('Select the type of the job', descriptions_type)

            # Find all date posted
            job_dates = page.find_all("div", {"data-facet":"date_posted","class":"eNr05b GbaVB ZkkK1e yUTMj k1U36b"})

            # Extract dates posted
            job_date_list = [job_date['data-name'] for job_date in job_dates if not job_date['data-value']=="__placeholder__"]
            
            selected_options_date = st.sidebar.radio('Select a date', job_date_list)

            #org filter
            org_filter=page.find_all('div',{'data-facet':"organization_mid","class":"eNr05b GbaVB ZkkK1e yUTMj k1U36b"})
            org_filter_name=[i['data-name'] for i in org_filter if not i['data-value']=='__placeholder__']
            #Select Organization filter if needed
            words_org = set(org_filter_name)
            descriptions_org = words_org
            
            selected_options_org = st.sidebar.multiselect('Select the organisation', descriptions_org)

            #Industry filter
            ind_filter=page.find_all('div',{'data-facet':"industry.id","class":"eNr05b GbaVB ZkkK1e yUTMj k1U36b"})
            ind_filter_name=[i['data-name'] for i in ind_filter if not i['data-value']=='__placeholder__']
            #Select Organization filter if needed
            words_ind = set(ind_filter_name)
            descriptions_ind = words_ind
            
            selected_options_ind = st.sidebar.multiselect('Select the organisation', descriptions_ind)

            filters='htichips='
            for i in selected_options_jobs:
                filters+=f'job_family_1:{i},'
            for i in selected_options_loc:
                filters+=f'city:{i},'
            for i in selected_options_type:
                filters+=f'employment_type:{i},'
            for i in selected_options_org:
                filters+=f'organization_mid:{i},'
            for i in selected_options_ind:
                filters+=f'industry.id:{i},'
            filters=filters[:-1]
                
            jobs_to_do = 1000
            jobs_done = 0
            l_prev=0
            
            browser.get(url)
            while jobs_done < jobs_to_do:
                lis = browser.find_elements(By.XPATH, "//li[@data-ved]//div[@role='treeitem']/div/div")
                l = len(lis)
                if l==0 or l==l_prev:
                    break
                li = lis[-1]
                browser.execute_script('arguments[0].scrollIntoView({block: "center", behavior: "smooth"});', li)        
                jobs_done = l
                print(f'{jobs_done=}', end='\r')
                time.sleep(2)
                l_prev = l
                
            page = BeautifulSoup(browser.page_source)
            list_of_jobs = page.find_all("div", {"class": "PwjeAc"})
            role = []
            company = []
            location = []
            description = []
            for i in list_of_jobs:
                role.append(i.select('div.BjJfJf.PUpOsf')[0].text)
                company.append(i.select('div.vNEEBe')[0].text)
                location.append(i.select('div.Qk80Jf')[0].text)
                elements = i.select('span.HBvzbc')
                if elements:
                    description.append(elements[0].text)
                else:
                    pass
            
            df=pd.DataFrame(zip(role,company,location,description),columns=['Role','Company','Location','Description'])
            
            with st.container():
                st.markdown("""
                            <style>
                            .css-1r6slb0.e1tzin5v1 {
                            background: #F0F2F6;
                            padding: 1.5rem;
                            box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1);
                            text-align: center;
                            border-radius: 0.25rem;
                            
                            }

                            .css-1wivap2.e16fv1kl3 {
                                color: #FF4B4B;
                            }
                            
                            .css-16idsys.e16nr0p34 {
                            color:  rgb(17 24 39 / 1) !important;
                        }
                        
                        .css-wvv94f.e16fv1kl2 {
                            justify-content: center !important;
                            display: flex !important;
                        }
                                </style>
                                """
                , unsafe_allow_html=True)
                
                col1, col2, col3 = st.columns(3)
                col1.metric("👩‍💼 Jobs Found", df.shape[0])
                col2.metric("💰Companies Found", df['Company'].nunique())
                col3.metric("📍Locations Available", df['Location'].nunique())
                
                
            with st.expander("View the Details here"):
                st.markdown("<br>", unsafe_allow_html=True)
                st.markdown("<h3>Jobs Available</h3>", unsafe_allow_html=True)
                st.markdown("<br>", unsafe_allow_html=True)
                st.dataframe(df)
            
    # Close the cursor and database connection
    cursor.close()
    db.close()

try:
    # Usage
    sidebar()
    main_content()
except:
    # Error
    with st.expander("Find out more"):
        st.markdown("**OH NO!!!**")
        st.markdown("We are sorry for this issue")
        st.markdown("We will fix it as soon as possible.")
        st.markdown("Until then please check out our other tools.")
        